{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All car brands that are met in the provided dataset\n",
    "car_brand = ['bmw', 'volkswagen', 'nissan', 'mercedes','toyota',\n",
    "         'audi', 'mitsubishi', 'skoda', 'volvo', 'honda',\n",
    "         'infiniti', 'lexus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script for scraping\n",
    "for brand in car_brand:\n",
    "    #Create a dictionary with all the functions to derive from a website\n",
    "    car_list = {'car_url': [], 'prod_year': [], 'mileage': [], 'body': [], 'color': [],\n",
    "               'motor': [], 'transmission': [], 'gear': [], 'stir': [], 'condition':[],\n",
    "               'owners': [], 'PTS': [], 'customs': [], 'price': []}\n",
    "    \n",
    "    pages_list = list(range(1, 11)) #Number of pages for scraping\n",
    "    \n",
    "    with tqdm(total=len(pages_list)) as pbar:\n",
    "        err=0 #Error counter\n",
    "        k=0   #Advertisement counter\n",
    "        \n",
    "        for i in pages_list:\n",
    "            url = f'https://auto.ru/cars/{brand}/used/?output_type=list&page={i}'\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            page = soup.find_all('a', class_ = 'Link ListingItemTitle-module__link')\n",
    "            links_list=[]  #Create a list of links to car advertisements\n",
    "            \n",
    "            for j in range(len(page)):\n",
    "                links_list.append(str(page[j]).split('\"')[3])\n",
    "                \n",
    "            for link in links_list:\n",
    "                k+=1\n",
    "                car_soup = BeautifulSoup(requests.get(link, headers={'User-Agent': 'Mozila/5.0'}).text, \n",
    "                                         'html.parser')\n",
    "                try:\n",
    "                    car_soup.find_all(\n",
    "                        'span', class_ = 'CardInfoRow__cell')[1].text.encode('l1').decode().replace('\\xa0', '')\n",
    "                    car_soup.find_all('span', class_ = 'OfferPriceCaption__price')[0]\n",
    "                except:\n",
    "                    err+=1\n",
    "                    print(k, link, err)\n",
    "                    print('Error')\n",
    "                    continue # continue if a car has been already sold and no price is mentioned on the page\n",
    "                car_list['car_url'].append(link)\n",
    "                l=1\n",
    "                \n",
    "                for key in list(car_list.keys())[1:-1]:\n",
    "                    car_list[key].append(car_soup.find_all(\n",
    "                        'span', class_ = 'CardInfoRow__cell')[l].text.encode('l1').decode().replace('\\xa0', ''))\n",
    "                    l+=2\n",
    "                    \n",
    "                car_list['price'].append(car_soup.find_all(\n",
    "                    'span', class_ = 'OfferPriceCaption__price')[0].text.encode('l1').decode().replace('\\xa0', ''))\n",
    "            pbar.update(1)\n",
    "            \n",
    "    a = pd.DataFrame.from_dict(car_list, orient='columns') # create a dataframe from the obtained dictionary\n",
    "    a.to_csv(f'{brand}.csv',index=False) # save the dataframe as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all dataframes in one dataset\n",
    "data = pd.read_csv(f'bmw.csv')\n",
    "for brand in car_brand[1::]:\n",
    "    brand = pd.read_csv(f'{brand}.csv')\n",
    "    data = pd.concat([data, brand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates\n",
    "data = data.drop_duplicates(data.drop_duplicates(subset=['prod_year',\n",
    " 'mileage',\n",
    " 'body',\n",
    " 'color',\n",
    " 'motor',\n",
    " 'transmission',\n",
    " 'gear',\n",
    " 'stir',\n",
    " 'condition',\n",
    " 'owners',\n",
    " 'PTS',\n",
    " 'customs',\n",
    " 'price'], inplace=True))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The obtained dataset does not contain any cars with body type = limousine.\n",
    "#Address this target.\n",
    "\n",
    "car_list = {'car_url': [], 'prod_year': [], 'mileage': [], 'body': [], 'color': [],\n",
    "            'motor': [], 'transmission': [], 'gear': [], 'stir': [], 'condition':[],\n",
    "            'owners': [], 'PTS': [], 'customs': [], 'price': []}\n",
    "pages_list = list(range(1, 2))\n",
    "\n",
    "with tqdm(total=len(pages_list)) as pbar:\n",
    "    err=0\n",
    "    k=0\n",
    "    url = f'https://auto.ru/cars/mercedes/used/?body_type_group=LIMOUSINE'\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    page = soup.find_all('a', class_ = 'Link ListingItemTitle-module__link')\n",
    "    links_list=[]\n",
    "    \n",
    "    for j in range(len(page)):\n",
    "        links_list.append(str(page[j]).split('\"')[3])\n",
    "        \n",
    "    for link in links_list:\n",
    "        k+=1\n",
    "        car_soup = BeautifulSoup(requests.get(link, headers={'User-Agent': 'Mozila/5.0'}).text, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            car_soup.find_all(\n",
    "                'span', class_ = 'CardInfoRow__cell')[1].text.encode('l1').decode().replace('\\xa0', '')\n",
    "            car_soup.find_all('span', class_ = 'OfferPriceCaption__price')[0]\n",
    "        except:\n",
    "            err+=1\n",
    "            prnit(k, link, err)\n",
    "            print('Error')\n",
    "            continue\n",
    "        car_list['car_url'].append(link)\n",
    "        l=1\n",
    "        \n",
    "        for key in list(car_list.keys())[1:-1]:\n",
    "            car_list[key].append(car_soup.find_all(\n",
    "                'span', class_ = 'CardInfoRow__cell')[l].text.encode('l1').decode().replace('\\xa0', ''))\n",
    "            l+=2\n",
    "            \n",
    "        car_list['price'].append(car_soup.find_all(\n",
    "            'span', class_ = 'OfferPriceCaption__price')[0].text.encode('l1').decode().replace('\\xa0', ''))\n",
    "    pbar.update(1)\n",
    "    \n",
    "a = pd.DataFrame.from_dict(car_list, orient='columns')\n",
    "a.to_csv(f'mercedes_limousine.csv',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
